{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweet_sentiment_service.model import SentimentModel\n",
    "from tweet_sentiment_service.inference import SentimentExtractor, SentimentID\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fastapi.testclient import TestClient\n",
    "from tweet_sentiment_service.app import app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Model Class only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 96\n",
    "PATH = './config/'\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    vocab=PATH+'vocab-roberta-base.json', \n",
    "    merges=PATH+'merges-roberta-base.txt', \n",
    "    lowercase=True,\n",
    "    add_prefix_space=True\n",
    ")\n",
    "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "test = pd.read_csv('./training/test.csv').fillna('')\n",
    "tweet = test.loc[2, 'text'] #'Last session of the day  http://twitpic.com/67ezh'\n",
    "sentiment = 'negative'\n",
    "ct = 1\n",
    "input_ids = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "# INPUT_IDS\n",
    "text1 = \" \"+\" \".join(tweet.split())\n",
    "enc = tokenizer.encode(text1)\n",
    "s_tok = sentiment_id[sentiment]           \n",
    "input_ids[0,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "attention_mask[0,:len(enc.ids)+5] = 1\n",
    "\n",
    "# Model Setup\n",
    "#model = SentimentModel(weights_path=\"./weights_final.h5\", config_path=PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7306,\n",
       " 478,\n",
       " 4342,\n",
       " 261,\n",
       " 5150,\n",
       " 5378,\n",
       " 42134,\n",
       " 13265,\n",
       " 6,\n",
       " 79,\n",
       " 34,\n",
       " 7,\n",
       " 6602,\n",
       " 69,\n",
       " 138,\n",
       " 6,\n",
       " 215,\n",
       " 10,\n",
       " 9208,\n",
       " 328]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text1).ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPLAY = 1\n",
    "predictions = model.predict(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "\n",
    "prediction_start = np.zeros((1,MAX_LEN))\n",
    "prediction_end = np.zeros((1,MAX_LEN))\n",
    "\n",
    "prediction_start += predictions[0]\n",
    "prediction_end += predictions[1]\n",
    "\n",
    "start_idx = np.argmax(prediction_start)\n",
    "end_idx = np.argmax(prediction_end)\n",
    "\n",
    "if start_idx > end_idx:\n",
    "    selected_text = tweet\n",
    "\n",
    "text = \" \"+\" \".join(tweet.split())\n",
    "encoded_text = tokenizer.encode(text)\n",
    "selected_text = tokenizer.decode(encoded_text.ids[start_idx-1:end_idx])\n",
    "\n",
    "print(selected_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Inference Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './config/'\n",
    "# Input\n",
    "test = pd.read_csv('./training/test.csv').fillna('')\n",
    "tweet = test.loc[2, 'text'] #'Last session of the day  http://twitpic.com/67ezh'\n",
    "sentiment = 'negative'\n",
    "\n",
    "# Inference\n",
    "sentiment_extractor = SentimentExtractor(weights_path=\"./weights_final.h5\", config_path=PATH)\n",
    "\n",
    "# Extract Sentiment from Tweet\n",
    "ans = sentiment_extractor.extract_sentiment(tweet=tweet, sentiment=sentiment)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = TestClient(app=app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 09:50:03.708786: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at ./config/pretrained-roberta-base.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 6s 6s/step\n"
     ]
    }
   ],
   "source": [
    "from tweet_sentiment_service.app import SentimentRequest\n",
    "data = {\n",
    "    \"tweet\": \"Recession hit Veronique Branquinho, she has to quit her company, such a shame!\",\n",
    "    \"sentiment\": \"negative\",\n",
    "}\n",
    "response = client.post(url=\"/sentiment\", json=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'selected_text': ' recession hit veronique branquinho, she has to quit her'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at ./config/pretrained-roberta-base.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 6s 6s/step\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"tweet\": \"I went on a walk with my dog yesterday, and saw a homeless guy. So sad!\",\n",
    "    \"sentiment\": \"negative\",\n",
    "}\n",
    "response = client.get(url=\"/sentiment\", params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'selected_text': 'I went on a walk with my dog yesterday, and saw a homeless guy. So sad!'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import *\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Hard Code some parameters of model\n",
    "MAX_LEN = 96\n",
    "DISPLAY = 1\n",
    "\n",
    "class SentimentModel:\n",
    "    def __init__(self, weights_path: str, config_path: str):\n",
    "        \"\"\"\n",
    "        Initializes the model and loads trained weights.\n",
    "\n",
    "        Args:\n",
    "            weights_path: Path to the trained weights for the model.\n",
    "            config_path: Path to the configuration files for the RoBERTa base model.\n",
    "        \"\"\"\n",
    "        self.max_len = MAX_LEN\n",
    "        self.config_roberta_base = config_path + \"config-roberta-base.json\"\n",
    "        self.pre_trained_roberta_weights = config_path + \"pretrained-roberta-base.h5\"\n",
    "\n",
    "        self.model = self.build_model()\n",
    "        self.model.load_weights(weights_path) \n",
    "\n",
    "    def build_model(self) -> tf.keras.Model: # type: ignore\n",
    "        \"\"\"\n",
    "        Builds the roBERTa based model to extract sentiment from tweets.\n",
    "        It uses a pre-trained roBERTa model and several convolutional and dropout layers\n",
    "        to extract sentiment from the text.\n",
    "\n",
    "        Returns:\n",
    "            tf.keras.Model: Compiled keras model.\n",
    "        \"\"\"\n",
    "        ids = tf.keras.layers.Input((self.max_len,), dtype=tf.int32)\n",
    "        att = tf.keras.layers.Input((self.max_len), dtype=tf.int32)\n",
    "        tok = tf.keras.layers.Input((self.max_len), dtype=tf.int32)\n",
    "\n",
    "        config = RobertaConfig.from_pretrained(self.config_roberta_base)\n",
    "        bert_model = TFRobertaModel.from_pretrained(self.pre_trained_roberta_weights, config=config)\n",
    "        x = bert_model(ids, attention_mask=att, token_type_ids=tok)\n",
    "        print(x)\n",
    "        x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "        x1 = tf.keras.layers.Conv1D(1,1)(x1)\n",
    "        x1 = tf.keras.layers.Flatten()(x1)\n",
    "        x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "\n",
    "\n",
    "        x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "        x2 = tf.keras.layers.Conv1D(1,1)(x2)\n",
    "        x2 = tf.keras.layers.Flatten()(x2)\n",
    "        x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "\n",
    "\n",
    "        model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    def predict(self, input_ids: np.ndarray, attention_mask: np.ndarray, token_type_ids: np.ndarray) -> str:\n",
    "        \"\"\"\n",
    "        Runs inference. It recieves the tokenized inputs and returns information about the start\n",
    "        and end indices of the sequence that represents the sentiment.\n",
    "\n",
    "        Args:\n",
    "            input_ids (np.ndarray): \n",
    "            attention_mask (np.ndarray): \n",
    "            token_type_ids (np.ndarray):\n",
    "\n",
    "        Returns:\n",
    "            str: Extracted sentiment of the tweet.\n",
    "        \"\"\"\n",
    "        preds = self.model.predict([input_ids, attention_mask, token_type_ids], verbose=DISPLAY)\n",
    "\n",
    "        return preds\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at ./config/pretrained-roberta-base.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFBaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=<KerasTensor: shape=(None, 96, 768) dtype=float32 (created by layer 'tf_roberta_model_1')>, pooler_output=<KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'tf_roberta_model_1')>, past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to find any matching files for weights_final",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./config/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mSentimentModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweights_final\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPATH\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 24\u001b[0m, in \u001b[0;36mSentimentModel.__init__\u001b[0;34m(self, weights_path, config_path)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_trained_roberta_weights \u001b[38;5;241m=\u001b[39m config_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretrained-roberta-base.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_model()\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.5/envs/tweet-sentiment-extraction/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.5/envs/tweet-sentiment-extraction/lib/python3.9/site-packages/tensorflow/python/training/py_checkpoint_reader.py:35\u001b[0m, in \u001b[0;36merror_translator\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     31\u001b[0m error_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot found in checkpoint\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_message \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to find any \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatching files for\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m error_message:\n\u001b[0;32m---> 35\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors_impl\u001b[38;5;241m.\u001b[39mNotFoundError(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, error_message)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSliced checkpoints are not supported\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_message \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData type \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupported\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m error_message:\n\u001b[1;32m     40\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors_impl\u001b[38;5;241m.\u001b[39mUnimplementedError(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, error_message)\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for weights_final"
     ]
    }
   ],
   "source": [
    "PATH = './config/'\n",
    "SentimentModel(weights_path=\"weights_final\", config_path=PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import tensorflow as tf\n",
    "from unittest.mock import MagicMock, patch\n",
    "from tokenizers import Encoding\n",
    "from tweet_sentiment_service.inference import SentimentExtractor, SentimentID\n",
    "from tweet_sentiment_service.model import SentimentModel\n",
    "\n",
    "\n",
    "MOCK_WEIGHTS_PATH = \"/path/to/mock_weights.h5\"\n",
    "MOCK_CONFIG_PATH = \"/path/to/mock_config/\"\n",
    "\n",
    "\n",
    "@pytest.fixture(scope=\"function\")\n",
    "def sentiment_model_mock(mocker):\n",
    "    # Mock the build_model method to return a tf.keras.Model\n",
    "    mocker.patch.object(SentimentModel, \"build_model\", return_value=MagicMock())\n",
    "    # Mock the load_weights method of the model instance\n",
    "    mocker.patch.object(tf.keras.Model, \"load_weights\", MagicMock())\n",
    "    # Mock init\n",
    "    mocker.patch.object(SentimentModel, \"__init__\", return_value=None)\n",
    "\n",
    "    return SentimentModel(weights_path=MOCK_WEIGHTS_PATH, config_path=MOCK_CONFIG_PATH)\n",
    "\n",
    "def test_initialization(mocker, sentiment_model_mock):\n",
    "    # Given\n",
    "\n",
    "    # Mock the build_model method to return a tf.keras.Model\n",
    "    build_model_mock = mocker.patch.object(SentimentModel, \"build_model\", return_value=MagicMock())\n",
    "    # Mock the load_weights method of the model instance\n",
    "    load_weights_mock = mocker.patch.object(tf.keras.Model, \"load_weights\", MagicMock())\n",
    "    # Mock tokenizer\n",
    "    tokenizer_mock = mocker.patch(\"tweet_sentiment_service.inference.ByteLevelBPETokenizer\", return_value=MagicMock())\n",
    "    # Mock SentimentModel init\n",
    "    model_init_mock = mocker.patch.object(SentimentModel, \"__init__\", return_value=None)\n",
    "    \n",
    "    # When\n",
    "\n",
    "    sentiment_extractor = SentimentExtractor(weights_path=MOCK_WEIGHTS_PATH, config_path=MOCK_CONFIG_PATH)\n",
    "\n",
    "    # Then\n",
    "\n",
    "    tokenizer_mock.assert_called_with(\n",
    "        vocab=f\"{MOCK_CONFIG_PATH}vocab-roberta-base.json\", \n",
    "        merges=f\"{MOCK_CONFIG_PATH}merges-roberta-base.txt\", \n",
    "        lowercase=True, \n",
    "        add_prefix_space=True\n",
    "    )\n",
    "\n",
    "    model_init_mock.assert_called_once_with(weights_path=MOCK_WEIGHTS_PATH, config_path=MOCK_CONFIG_PATH)\n",
    "    print(\"success\")\n",
    "\n",
    "\n",
    "test_initialization(mocker, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tokenize_and_mask(mocker):\n",
    "    # Given\n",
    "    tweet = \"This is a happy test tweet.\"\n",
    "    sentiment = \"positive\"\n",
    "    build_model_mock = mocker.patch.object(SentimentModel, \"build_model\", return_value=MagicMock())\n",
    "    load_weights_mock = mocker.patch.object(tf.keras.Model, \"load_weights\", MagicMock())\n",
    "    tokenizer_mock = mocker.patch(\"tweet_sentiment_service.inference.ByteLevelBPETokenizer\", return_value=MagicMock())\n",
    "    tokenizer_mock = mocker.patch(\"tweet_sentiment_service.inference.ByteLevelBPETokenizer\", return_value=MagicMock())\n",
    "    tokenizer_mock.encode.ids.return_value = [10, 11, 12]\n",
    "    sentiment_extractor = SentimentExtractor(MOCK_WEIGHTS_PATH, MOCK_CONFIG_PATH)\n",
    "\n",
    "    # When\n",
    "    input_ids, attention_mask, token_type_ids = sentiment_extractor.tokenize_and_mask(tweet, sentiment)\n",
    "\n",
    "    # Then\n",
    "    assert input_ids.shape == (1, sentiment_extractor.model.max_len)\n",
    "    assert attention_mask.shape == (1, sentiment_extractor.model.max_len)\n",
    "    assert token_type_ids.shape == (1, sentiment_extractor.model.max_len)\n",
    "\n",
    "    assert input_ids[0, 0] == 0\n",
    "    assert input_ids[0, 4] == SentimentID.POSITIVE.value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweet-sentiment-extraction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
